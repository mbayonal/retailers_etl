{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "\n",
    "This notebook contains tools and examples for data science and engineering using PySpark, Pandas, Polars, Scikit-learn, NumPy, and other libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove deprecated packages\n",
    "# Basic data manipulation and analysis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ydata_profiling import ProfileReport, compare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Spark Session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing Parquet files\n",
    "data_dir = \"/home/manuel-bayona/Documents/crisil2/data/\"\n",
    "\n",
    "# List of Parquet file names\n",
    "parquet_files = [\n",
    "    \"cubesmart_weekly_20250127_181344.parquet\",\n",
    "    \"cubesmart_weekly_20250203_181358.parquet\",\n",
    "    \"cubesmart_weekly_20250210_061336.parquet\",\n",
    "    \"smartstopselfstorage_weekly_20250127_090106.parquet\",\n",
    "    \"smartstopselfstorage_weekly_20250203_090120.parquet\",\n",
    "    \"smartstopselfstorage_weekly_20250210_060127.parquet\",\n",
    "    \"storagemart_weekly_20250127_090843.parquet\",\n",
    "    \"storagemart_weekly_20250203_091401.parquet\",\n",
    "    \"storagemart_weekly_20250210_061303.parquet\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema extracted for file: cubesmart_weekly_20250127_181344.parquet\n",
      "Schema extracted for file: cubesmart_weekly_20250203_181358.parquet\n",
      "Schema extracted for file: cubesmart_weekly_20250210_061336.parquet\n",
      "Schema extracted for file: smartstopselfstorage_weekly_20250127_090106.parquet\n",
      "Schema extracted for file: smartstopselfstorage_weekly_20250203_090120.parquet\n",
      "Schema extracted for file: smartstopselfstorage_weekly_20250210_060127.parquet\n",
      "Schema extracted for file: storagemart_weekly_20250127_090843.parquet\n",
      "Schema extracted for file: storagemart_weekly_20250203_091401.parquet\n",
      "Schema extracted for file: storagemart_weekly_20250210_061303.parquet\n",
      "\n",
      "=== Grouped Files by Schema ===\n",
      "\n",
      "Group 1: 6 file(s)\n",
      "  - cubesmart_weekly_20250127_181344.parquet\n",
      "  - cubesmart_weekly_20250203_181358.parquet\n",
      "  - cubesmart_weekly_20250210_061336.parquet\n",
      "  - storagemart_weekly_20250127_090843.parquet\n",
      "  - storagemart_weekly_20250203_091401.parquet\n",
      "  - storagemart_weekly_20250210_061303.parquet\n",
      "Schema:\n",
      "  Run_Date: datetime64[ns]\n",
      "  Retailer: object\n",
      "  Retailer_Website: object\n",
      "  Store_Info: object\n",
      "  Category_Traversal: object\n",
      "  Brand: object\n",
      "  Product_Name: object\n",
      "  Product_ID: object\n",
      "  Universal_Product_ID: object\n",
      "  Variant_Info: object\n",
      "  Size_Or_Quantity: object\n",
      "  Number_Of_Reviews: float64\n",
      "  Average_Rating: object\n",
      "  Original_Price: object\n",
      "  Sale_Price: object\n",
      "  Currency: object\n",
      "  Url: object\n",
      "  In_Stock: bool\n",
      "\n",
      "Group 2: 3 file(s)\n",
      "  - smartstopselfstorage_weekly_20250127_090106.parquet\n",
      "  - smartstopselfstorage_weekly_20250203_090120.parquet\n",
      "  - smartstopselfstorage_weekly_20250210_060127.parquet\n",
      "Schema:\n",
      "  Run_Date: datetime64[ns]\n",
      "  Retailer: object\n",
      "  Retailer_Website: object\n",
      "  Store_Info: object\n",
      "  Category_Traversal: object\n",
      "  Brand: object\n",
      "  Product_Name: object\n",
      "  Product_ID: object\n",
      "  Universal_Product_ID: object\n",
      "  Variant_Info: object\n",
      "  Size_Or_Quantity: object\n",
      "  Number_Of_Reviews: float64\n",
      "  Average_Rating: float64\n",
      "  Original_Price: float64\n",
      "  Sale_Price: float64\n",
      "  Currency: object\n",
      "  Url: object\n",
      "  In_Stock: bool\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "# Store schema per file\n",
    "file_schemas = {}\n",
    "\n",
    "# Load each file and store its schema\n",
    "for file in parquet_files:\n",
    "    file_path = os.path.join(data_dir, file)\n",
    "    try:\n",
    "        df = pd.read_parquet(file_path)\n",
    "        schema = tuple((col, str(dtype)) for col, dtype in df.dtypes.items())\n",
    "        file_schemas[file] = schema\n",
    "        print(f\"Schema extracted for file: {file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load Parquet file {file}: {e}\")\n",
    "\n",
    "# Group files with identical schemas\n",
    "schema_groups = defaultdict(list)\n",
    "for file, schema in file_schemas.items():\n",
    "    schema_groups[schema].append(file)\n",
    "\n",
    "# Output grouped schemas for manual field mapping\n",
    "print(\"\\n=== Grouped Files by Schema ===\")\n",
    "for idx, (schema, files) in enumerate(schema_groups.items(), start=1):\n",
    "    print(f\"\\nGroup {idx}: {len(files)} file(s)\")\n",
    "    for f in files:\n",
    "        print(f\"  - {f}\")\n",
    "    print(\"Schema:\")\n",
    "    for col, dtype in schema:\n",
    "        print(f\"  {col}: {dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Schema Group Analysis of Parquet Files\n",
    "\n",
    "This notebook analyzes the schema structures across 9 Parquet files from 3 different retailers. The goal is to identify similarities and differences in schema definitions, which is a critical step before performing unified data transformations or ingestion into a central data platform.\n",
    "\n",
    "---\n",
    "\n",
    "## Group Overview\n",
    "\n",
    "The analysis grouped the files into two schema groups based on exact matches in field names and data types.\n",
    "\n",
    "### **Group 1** (6 files)\n",
    "- **Retailers**: `cubesmart`, `storagemart`\n",
    "- **Files**:\n",
    "  - `cubesmart_weekly_20250127_181344.parquet`\n",
    "  - `cubesmart_weekly_20250203_181358.parquet`\n",
    "  - `cubesmart_weekly_20250210_061336.parquet`\n",
    "  - `storagemart_weekly_20250127_090843.parquet`\n",
    "  - `storagemart_weekly_20250203_091401.parquet`\n",
    "  - `storagemart_weekly_20250210_061303.parquet`\n",
    "\n",
    "**Schema Fields**:\n",
    "- `Run_Date`: datetime64[ns]  \n",
    "- `Retailer`: object  \n",
    "- `Retailer_Website`: object  \n",
    "- `Store_Info`: object  \n",
    "- `Category_Traversal`: object  \n",
    "- `Brand`: object  \n",
    "- `Product_Name`: object  \n",
    "- `Product_ID`: object  \n",
    "- `Universal_Product_ID`: object  \n",
    "- `Variant_Info`: object  \n",
    "- `Size_Or_Quantity`: object  \n",
    "- `Number_Of_Reviews`: float64  \n",
    "- `Average_Rating`: object  \n",
    "- `Original_Price`: object  \n",
    "- `Sale_Price`: object  \n",
    "- `Currency`: object  \n",
    "- `Url`: object  \n",
    "- `In_Stock`: bool  \n",
    "\n",
    "---\n",
    "\n",
    "### **Group 2** (3 files)\n",
    "- **Retailer**: `smartstopselfstorage`\n",
    "- **Files**:\n",
    "  - `smartstopselfstorage_weekly_20250127_090106.parquet`\n",
    "  - `smartstopselfstorage_weekly_20250203_090120.parquet`\n",
    "  - `smartstopselfstorage_weekly_20250210_060127.parquet`\n",
    "\n",
    "**Schema Fields**:\n",
    "- `Run_Date`: datetime64[ns]  \n",
    "- `Retailer`: object  \n",
    "- `Retailer_Website`: object  \n",
    "- `Store_Info`: object  \n",
    "- `Category_Traversal`: object  \n",
    "- `Brand`: object  \n",
    "- `Product_Name`: object  \n",
    "- `Product_ID`: object  \n",
    "- `Universal_Product_ID`: object  \n",
    "- `Variant_Info`: object  \n",
    "- `Size_Or_Quantity`: object  \n",
    "- `Number_Of_Reviews`: float64  \n",
    "- `Average_Rating`: float64  \n",
    "- `Original_Price`: float64  \n",
    "- `Sale_Price`: float64  \n",
    "- `Currency`: object  \n",
    "- `Url`: object  \n",
    "- `In_Stock`: bool  \n",
    "\n",
    "---\n",
    "\n",
    "## Key Differences\n",
    "\n",
    "Although all files share the same field names and structure, the following fields differ in data type:\n",
    "\n",
    "| Field              | Group 1 Type | Group 2 Type |\n",
    "|-------------------|--------------|--------------|\n",
    "| `Average_Rating`  | object       | float64      |\n",
    "| `Original_Price`  | object       | float64      |\n",
    "| `Sale_Price`      | object       | float64      |\n",
    "\n",
    "These differences indicate inconsistencies in how numeric values are stored. Group 1 files likely contain string representations of numbers (possibly with currency symbols or invalid formats), while Group 2 files have clean numeric values.\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "The schema is structurally aligned across all files but differs in type quality for numeric fields. This should be addressed in preprocessing steps to ensure consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define canonical column types\n",
    "canonical_dtypes = {\n",
    "    \"Run_Date\": \"datetime64[ns]\",\n",
    "    \"Retailer\": \"string\",\n",
    "    \"Retailer_Website\": \"string\",\n",
    "    \"Store_Info\": \"string\",\n",
    "    \"Category_Traversal\": \"string\",\n",
    "    \"Brand\": \"string\",\n",
    "    \"Product_Name\": \"string\",\n",
    "    \"Product_ID\": \"string\",\n",
    "    \"Universal_Product_ID\": \"string\",\n",
    "    \"Variant_Info\": \"string\",\n",
    "    \"Size_Or_Quantity\": \"string\",\n",
    "    \"Number_Of_Reviews\": \"float64\",\n",
    "    \"Average_Rating\": \"float64\",\n",
    "    \"Original_Price\": \"float64\",\n",
    "    \"Sale_Price\": \"float64\",\n",
    "    \"Currency\": \"string\",\n",
    "    \"Url\": \"string\",\n",
    "    \"In_Stock\": \"boolean\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_normalize_parquet(file_path: str, dtypes: dict) -> pd.DataFrame:\n",
    "    df = pd.read_parquet(file_path)\n",
    "\n",
    "    # Coerce columns to canonical types\n",
    "    for column, target_dtype in dtypes.items():\n",
    "        if column not in df.columns:\n",
    "            df[column] = pd.NA\n",
    "        try:\n",
    "            if \"float\" in target_dtype:\n",
    "                df[column] = pd.to_numeric(df[column], errors=\"coerce\")\n",
    "            elif \"datetime\" in target_dtype:\n",
    "                df[column] = pd.to_datetime(df[column], errors=\"coerce\")\n",
    "            elif target_dtype == \"boolean\":\n",
    "                df[column] = df[column].astype(\"boolean\")\n",
    "            else:\n",
    "                df[column] = df[column].astype(\"string\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Failed to cast column {column} in {os.path.basename(file_path)}: {e}\")\n",
    "    \n",
    "    # Ensure all canonical columns exist\n",
    "    for col in dtypes:\n",
    "        if col not in df.columns:\n",
    "            df[col] = pd.NA\n",
    "\n",
    "    return df[dtypes.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#handle duplicated cells\n",
    "def load_and_normalize_parquet(file_path: str, dtypes: dict) -> pd.DataFrame:\n",
    "    df = pd.read_parquet(file_path)\n",
    "\n",
    "    # Coerce columns to canonical types\n",
    "    for column, target_dtype in dtypes.items():\n",
    "        if column not in df.columns:\n",
    "            df[column] = pd.NA\n",
    "        try:\n",
    "            if \"float\" in target_dtype:\n",
    "                df[column] = pd.to_numeric(df[column], errors=\"coerce\")\n",
    "            elif \"datetime\" in target_dtype:\n",
    "                df[column] = pd.to_datetime(df[column], errors=\"coerce\")\n",
    "            elif target_dtype == \"boolean\":\n",
    "                df[column] = df[column].astype(\"boolean\")\n",
    "            else:\n",
    "                df[column] = df[column].astype(\"string\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Failed to cast column {column} in {os.path.basename(file_path)}: {e}\")\n",
    "    \n",
    "    # Ensure all canonical columns exist\n",
    "    for col in dtypes:\n",
    "        if col not in df.columns:\n",
    "            df[col] = pd.NA\n",
    "\n",
    "    return df[dtypes.keys()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "consolidated_data = []\n",
    "\n",
    "for file_name in parquet_files:\n",
    "    file_path = os.path.join(data_dir, file_name)\n",
    "    try:\n",
    "        df = load_and_normalize_parquet(file_path, canonical_dtypes)\n",
    "        df[\"Source_File\"] = file_name  # Optional: keep source traceability\n",
    "        consolidated_data.append(df)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process {file_name}: {e}\")\n",
    "\n",
    "# Final concatenated dataframe\n",
    "unified_df = pd.concat(consolidated_data, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Run_Date</th>\n",
       "      <th>Retailer</th>\n",
       "      <th>Retailer_Website</th>\n",
       "      <th>Store_Info</th>\n",
       "      <th>Category_Traversal</th>\n",
       "      <th>Brand</th>\n",
       "      <th>Product_Name</th>\n",
       "      <th>Product_ID</th>\n",
       "      <th>Universal_Product_ID</th>\n",
       "      <th>Variant_Info</th>\n",
       "      <th>Size_Or_Quantity</th>\n",
       "      <th>Number_Of_Reviews</th>\n",
       "      <th>Average_Rating</th>\n",
       "      <th>Original_Price</th>\n",
       "      <th>Sale_Price</th>\n",
       "      <th>Currency</th>\n",
       "      <th>Url</th>\n",
       "      <th>In_Stock</th>\n",
       "      <th>Source_File</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-01-27 17:00:21</td>\n",
       "      <td>CubeSmart</td>\n",
       "      <td>cubesmart.com</td>\n",
       "      <td>{\"Store_Id\":\"3222\",\"Store_Name\":\"CubeSmart of ...</td>\n",
       "      <td>CubeSmart Self Storage &gt; Maryland Self Storage...</td>\n",
       "      <td>CubeSmart</td>\n",
       "      <td>Large</td>\n",
       "      <td>6efcc51c-879f-4914-bf62-7fe9e49a91d3</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>{\"Features\":\"Climate controlled, Elevator acce...</td>\n",
       "      <td>20'x10'</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>437.0</td>\n",
       "      <td>262.2</td>\n",
       "      <td>USD</td>\n",
       "      <td>https://www.cubesmart.com/maryland-self-storag...</td>\n",
       "      <td>True</td>\n",
       "      <td>cubesmart_weekly_20250127_181344.parquet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-01-27 17:00:21</td>\n",
       "      <td>CubeSmart</td>\n",
       "      <td>cubesmart.com</td>\n",
       "      <td>{\"Store_Id\":\"4245\",\"Store_Name\":\"CubeSmart of ...</td>\n",
       "      <td>CubeSmart Self Storage &gt; Arizona Self Storage ...</td>\n",
       "      <td>CubeSmart</td>\n",
       "      <td>Large</td>\n",
       "      <td>6db3e08a-6122-46ff-b66f-eec08fdd5c3c</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>{\"Features\":\"Climate controlled, Elevator acce...</td>\n",
       "      <td>10'x20'</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>273.0</td>\n",
       "      <td>163.8</td>\n",
       "      <td>USD</td>\n",
       "      <td>https://www.cubesmart.com/arizona-self-storage...</td>\n",
       "      <td>True</td>\n",
       "      <td>cubesmart_weekly_20250127_181344.parquet</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Run_Date   Retailer Retailer_Website  \\\n",
       "0 2025-01-27 17:00:21  CubeSmart    cubesmart.com   \n",
       "1 2025-01-27 17:00:21  CubeSmart    cubesmart.com   \n",
       "\n",
       "                                          Store_Info  \\\n",
       "0  {\"Store_Id\":\"3222\",\"Store_Name\":\"CubeSmart of ...   \n",
       "1  {\"Store_Id\":\"4245\",\"Store_Name\":\"CubeSmart of ...   \n",
       "\n",
       "                                  Category_Traversal      Brand Product_Name  \\\n",
       "0  CubeSmart Self Storage > Maryland Self Storage...  CubeSmart        Large   \n",
       "1  CubeSmart Self Storage > Arizona Self Storage ...  CubeSmart        Large   \n",
       "\n",
       "                             Product_ID Universal_Product_ID  \\\n",
       "0  6efcc51c-879f-4914-bf62-7fe9e49a91d3                 <NA>   \n",
       "1  6db3e08a-6122-46ff-b66f-eec08fdd5c3c                 <NA>   \n",
       "\n",
       "                                        Variant_Info Size_Or_Quantity  \\\n",
       "0  {\"Features\":\"Climate controlled, Elevator acce...          20'x10'   \n",
       "1  {\"Features\":\"Climate controlled, Elevator acce...          10'x20'   \n",
       "\n",
       "   Number_Of_Reviews  Average_Rating  Original_Price  Sale_Price Currency  \\\n",
       "0                NaN             NaN           437.0       262.2      USD   \n",
       "1                NaN             NaN           273.0       163.8      USD   \n",
       "\n",
       "                                                 Url  In_Stock  \\\n",
       "0  https://www.cubesmart.com/maryland-self-storag...      True   \n",
       "1  https://www.cubesmart.com/arizona-self-storage...      True   \n",
       "\n",
       "                                Source_File  \n",
       "0  cubesmart_weekly_20250127_181344.parquet  \n",
       "1  cubesmart_weekly_20250127_181344.parquet  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unified_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Paola Otero\\anaconda3\\Lib\\site-packages\\ydata_profiling\\profile_report.py:365: UserWarning: Try running command: 'pip install --upgrade Pillow' to avoid ValueError\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b95bf31b3f04396a8f3ea1b4bd52a24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Summarize dataset:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                           | 0/19 [00:00<?, ?it/s]\u001b[A\n",
      "  5%|████▎                                                                              | 1/19 [00:00<00:14,  1.23it/s]\u001b[A\n",
      " 11%|████████▋                                                                          | 2/19 [00:01<00:13,  1.23it/s]\u001b[A\n",
      " 16%|█████████████                                                                      | 3/19 [00:02<00:11,  1.37it/s]\u001b[A\n",
      " 21%|█████████████████▍                                                                 | 4/19 [00:02<00:08,  1.82it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 19/19 [00:03<00:00,  5.76it/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "791c144573d24bf48a8e811dc3354dd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generate report structure:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77769abcae1e4b5ca3d16f5ecfd3df80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Render HTML:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "233fb72a43884c8f974834145148867f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Export report to file:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "report = ProfileReport(df = unified_df, title = \"data profiling report\")\n",
    "report.to_file(\"crisil_report.html\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CRISIL Data Profiling Report – Key Insights\n",
    "\n",
    "## 1. Data Quality Issues\n",
    "\n",
    "### Missing Data\n",
    "- `Number_Of_Reviews`: 100% missing\n",
    "- `Average_Rating`: 100% missing\n",
    "- `Universal_Product_ID`: 80.2% missing\n",
    "- `Original_Price`: 12.1% missing\n",
    "\n",
    "### Unsupported Types\n",
    "- `Number_Of_Reviews` and `Average_Rating` are marked as **unsupported**, indicating potential issues with data type parsing or formatting. These fields may require cleaning or conversion.\n",
    "\n",
    "### Constant Columns\n",
    "- `Universal_Product_ID` has a constant empty value across all records.\n",
    "- `Currency` has a constant value: `\"USD\"`.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. High Correlation Between Features\n",
    "\n",
    "The following features exhibit **high correlation**, which may introduce redundancy or multicollinearity if ML models are trained:\n",
    "\n",
    "- `Brand` is highly correlated with:\n",
    "  - `In_Stock`\n",
    "  - `Retailer`\n",
    "  - `Retailer_Website`\n",
    "  - `Source_File`\n",
    "  - `Product_Name`\n",
    "  \n",
    "- `In_Stock` shows the same correlation pattern as `Brand`.\n",
    "- `Original_Price` is highly correlated with `Sale_Price`.\n",
    "- `Retailer_Website` and `Source_File` are also correlated with `Brand` and its associated fields.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Dataset Summary\n",
    "\n",
    "- **Number of Observations**: 99,595\n",
    "- **Number of Variables**: 19\n",
    "- **Missing Cells**: 291,117 (15.4%)\n",
    "- **Duplicate Rows**: 0 (0.0%)\n",
    "- **Total Memory Usage**: 13.9 MiB\n",
    "- **Average Record Size**: 146.0 Bytes\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Variable Type Breakdown\n",
    "\n",
    "- **Numeric**: 2\n",
    "- **Categorical**: 7\n",
    "- **Text**: 6\n",
    "- **Boolean**: 1\n",
    "- **DateTime**: 1\n",
    "- **Unsupported**: 2\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Notes for Further Action\n",
    "\n",
    "- Investigate the reason behind 100% missing values in `Number_Of_Reviews` and `Average_Rating`.\n",
    "- Validate whether constant columns such as `Currency` and `Universal_Product_ID` add value to downstream tasks.\n",
    "- Consider dimensionality reduction or feature selection to handle high correlation among multiple columns.\n",
    "- Clean or transform unsupported columns to align with the expected data types.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Field Criticality and Data Quality Constraints\n",
    "\n",
    "The following analysis classifies each field in the dataset according to its role in supporting business operations, analytical tasks, and machine learning models, particularly for self-storage product offerings. Each field is evaluated based on:\n",
    "\n",
    "- **Operational Criticality**: Importance for core business processes such as inventory management and availability tracking.\n",
    "- **Analytical Value**: Usefulness for descriptive analytics, reporting, and exploration.\n",
    "- **Modeling Impact**: Influence on predictive models, especially recommendation systems or pricing algorithms.\n",
    "\n",
    "The **criticality matrix** establishes a foundation for defining data quality standards. Based on this classification, a set of **field-level constraints** has been specified to ensure that the most important fields meet minimum data completeness and validity thresholds. These constraints aim to safeguard downstream analytical reliability and model performance in production contexts.\n",
    "\n",
    "## Criticality Matrix\n",
    "\n",
    "| Field                 | Operational Criticality | Analytical Value | Modeling Impact | Notes |\n",
    "|----------------------|--------------------------|------------------|------------------|-------|\n",
    "| Run_Date             | High                     | High             | High             | Useful for tracking updates and time-based trends. |\n",
    "| Retailer             | High                     | High             | High             | Essential for brand-level grouping and filtering. |\n",
    "| Retailer_Website     | Medium                   | Low              | Low              | Mostly metadata; may support traceability. |\n",
    "| Store_Info           | High                     | High             | Medium           | Contains location metadata critical for routing and geo-segmentation. |\n",
    "| Category_Traversal   | Medium                   | High             | Medium           | Useful for categorization and taxonomy-level recommendations. |\n",
    "| Brand                | High                     | Medium           | Medium           | Redundant with Retailer in this case, but still relevant for display and filters. |\n",
    "| Product_Name         | High                     | Medium           | Medium           | Describes the unit; may help enrich feature sets. |\n",
    "| Product_ID           | High                     | Medium           | High             | Unique identifier, required for joins, matching, and recommendations. |\n",
    "| Universal_Product_ID | Low (currently)          | Low              | Low              | High null rate; not currently usable. |\n",
    "| Variant_Info         | Medium                   | High             | Medium           | Holds detailed features; may need parsing but valuable in modeling. |\n",
    "| Size_Or_Quantity     | High                     | High             | High             | Core attribute used in search filters and pricing models. |\n",
    "| Number_Of_Reviews    | Low (currently)          | High             | High             | 100% missing but critical for trust and ranking models if populated. |\n",
    "| Average_Rating       | Low (currently)          | High             | High             | Similar to above; highly influential in recommendations. |\n",
    "| Original_Price       | High                     | High             | High             | Used in pricing logic and value-based modeling. |\n",
    "| Sale_Price           | High                     | High             | High             | Represents the effective price; essential for promotion tracking and optimization. |\n",
    "| Currency             | Medium                   | Low              | Low              | Constant field (USD); may be dropped if invariant. |\n",
    "| Url                  | Medium                   | Low              | Low              | Useful for traceability or display but not for modeling. |\n",
    "| In_Stock             | High                     | High             | High             | Core operational signal; likely to influence availability-based ranking. |\n",
    "\n",
    "## Updated Data Constraints for Critical Features\n",
    "\n",
    "| Field              | Constraint Type            | Threshold / Range                    | Justification |\n",
    "|-------------------|----------------------------|--------------------------------------|---------------|\n",
    "| Run_Date           | Missing Values             | ≤ 0.5%                               | Required for temporal freshness tracking and data versioning. |\n",
    "| Retailer           | Missing Values             | ≤ 0.5%                               | Drives brand-level aggregation and analytics. |\n",
    "| Store_Info         | JSON Parse Success         | 100% valid JSON                      | Structured parsing is necessary for location and feature extraction. |\n",
    "| Category_Traversal | Missing Values             | ≤ 2%                                 | Important for taxonomy-based grouping; moderate tolerance. |\n",
    "| Brand              | Missing Values             | ≤ 1%                                 | Critical for display, filtering, and modeling redundancy with Retailer. |\n",
    "| Product_Name       | Missing Values             | ≤ 1%                                 | Needed for customer-facing naming logic and modeling context. |\n",
    "| Product_ID         | Uniqueness                 | 100% unique per retailer             | Acts as the unique key across data systems and model features. |\n",
    "| Variant_Info       | JSON Parse Success         | ≥ 95% valid                          | Enables extraction of product features; partial tolerance allowed. |\n",
    "| Size_Or_Quantity   | Missing Values             | ≤ 1%                                 | Crucial for dimensional categorization and pricing. |\n",
    "| Original_Price     | Missing Values             | ≤ 2%                                 | Required for discount logic and price sensitivity modeling. |\n",
    "|                    | Min/Max Value              | ≥ 5 and ≤ 5760 (with 20% tolerance)  | Observed range: 5–4800; tolerance allows for future pricing shifts. |\n",
    "| Sale_Price         | Missing Values             | ≤ 2%                                 | Core business metric; used in ranking and value-based recommendation. |\n",
    "|                    | Min/Max Value              | ≥ 3 and ≤ 9644.4 (with 20% tolerance)| Observed range: 3–8037; tolerance accounts for future data packages. |\n",
    "| In_Stock           | Missing Values             | ≤ 0.5%                               | Required for availability filtering and operational logic. |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
