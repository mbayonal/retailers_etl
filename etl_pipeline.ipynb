{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "A module that was compiled using NumPy 1.x cannot be run in\n",
            "NumPy 2.2.4 as it may crash. To support both 1.x and 2.x\n",
            "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
            "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
            "\n",
            "If you are a user of the module, the easiest solution will be to\n",
            "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
            "We expect that some modules will need time to support NumPy 2.\n",
            "\n",
            "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/home/manuel-bayona/Documents/crisil2/venv/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/home/manuel-bayona/Documents/crisil2/venv/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
            "    app.start()\n",
            "  File \"/home/manuel-bayona/Documents/crisil2/venv/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/home/manuel-bayona/Documents/crisil2/venv/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/home/manuel-bayona/anaconda3/lib/python3.12/asyncio/base_events.py\", line 641, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/home/manuel-bayona/anaconda3/lib/python3.12/asyncio/base_events.py\", line 1986, in _run_once\n",
            "    handle._run()\n",
            "  File \"/home/manuel-bayona/anaconda3/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/home/manuel-bayona/Documents/crisil2/venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
            "    await self.process_one()\n",
            "  File \"/home/manuel-bayona/Documents/crisil2/venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
            "    await dispatch(*args)\n",
            "  File \"/home/manuel-bayona/Documents/crisil2/venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
            "    await result\n",
            "  File \"/home/manuel-bayona/Documents/crisil2/venv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
            "    await super().execute_request(stream, ident, parent)\n",
            "  File \"/home/manuel-bayona/Documents/crisil2/venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
            "    reply_content = await reply_content\n",
            "  File \"/home/manuel-bayona/Documents/crisil2/venv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
            "    res = shell.run_cell(\n",
            "  File \"/home/manuel-bayona/Documents/crisil2/venv/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
            "    return super().run_cell(*args, **kwargs)\n",
            "  File \"/home/manuel-bayona/Documents/crisil2/venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3047, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/home/manuel-bayona/Documents/crisil2/venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3102, in _run_cell\n",
            "    result = runner(coro)\n",
            "  File \"/home/manuel-bayona/Documents/crisil2/venv/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/home/manuel-bayona/Documents/crisil2/venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3306, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/home/manuel-bayona/Documents/crisil2/venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3489, in run_ast_nodes\n",
            "    if await self.run_code(code, result, async_=asy):\n",
            "  File \"/home/manuel-bayona/Documents/crisil2/venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3549, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"/tmp/ipykernel_4509/208977101.py\", line 12, in <module>\n",
            "    import pandas as pd\n",
            "  File \"/home/manuel-bayona/Documents/crisil2/venv/lib/python3.12/site-packages/pandas/__init__.py\", line 26, in <module>\n",
            "    from pandas.compat import (\n",
            "  File \"/home/manuel-bayona/Documents/crisil2/venv/lib/python3.12/site-packages/pandas/compat/__init__.py\", line 27, in <module>\n",
            "    from pandas.compat.pyarrow import (\n",
            "  File \"/home/manuel-bayona/Documents/crisil2/venv/lib/python3.12/site-packages/pandas/compat/pyarrow.py\", line 8, in <module>\n",
            "    import pyarrow as pa\n",
            "  File \"/home/manuel-bayona/Documents/crisil2/venv/lib/python3.12/site-packages/pyarrow/__init__.py\", line 65, in <module>\n",
            "    import pyarrow.lib as _lib\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "_ARRAY_API not found",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
            "\u001b[31mAttributeError\u001b[39m: _ARRAY_API not found"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "A module that was compiled using NumPy 1.x cannot be run in\n",
            "NumPy 2.2.4 as it may crash. To support both 1.x and 2.x\n",
            "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
            "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
            "\n",
            "If you are a user of the module, the easiest solution will be to\n",
            "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
            "We expect that some modules will need time to support NumPy 2.\n",
            "\n",
            "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/home/manuel-bayona/Documents/crisil2/venv/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/home/manuel-bayona/Documents/crisil2/venv/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
            "    app.start()\n",
            "  File \"/home/manuel-bayona/Documents/crisil2/venv/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/home/manuel-bayona/Documents/crisil2/venv/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/home/manuel-bayona/anaconda3/lib/python3.12/asyncio/base_events.py\", line 641, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/home/manuel-bayona/anaconda3/lib/python3.12/asyncio/base_events.py\", line 1986, in _run_once\n",
            "    handle._run()\n",
            "  File \"/home/manuel-bayona/anaconda3/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/home/manuel-bayona/Documents/crisil2/venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
            "    await self.process_one()\n",
            "  File \"/home/manuel-bayona/Documents/crisil2/venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
            "    await dispatch(*args)\n",
            "  File \"/home/manuel-bayona/Documents/crisil2/venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
            "    await result\n",
            "  File \"/home/manuel-bayona/Documents/crisil2/venv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
            "    await super().execute_request(stream, ident, parent)\n",
            "  File \"/home/manuel-bayona/Documents/crisil2/venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
            "    reply_content = await reply_content\n",
            "  File \"/home/manuel-bayona/Documents/crisil2/venv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
            "    res = shell.run_cell(\n",
            "  File \"/home/manuel-bayona/Documents/crisil2/venv/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
            "    return super().run_cell(*args, **kwargs)\n",
            "  File \"/home/manuel-bayona/Documents/crisil2/venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3047, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/home/manuel-bayona/Documents/crisil2/venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3102, in _run_cell\n",
            "    result = runner(coro)\n",
            "  File \"/home/manuel-bayona/Documents/crisil2/venv/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/home/manuel-bayona/Documents/crisil2/venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3306, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/home/manuel-bayona/Documents/crisil2/venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3489, in run_ast_nodes\n",
            "    if await self.run_code(code, result, async_=asy):\n",
            "  File \"/home/manuel-bayona/Documents/crisil2/venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3549, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"/tmp/ipykernel_4509/208977101.py\", line 12, in <module>\n",
            "    import pandas as pd\n",
            "  File \"/home/manuel-bayona/Documents/crisil2/venv/lib/python3.12/site-packages/pandas/__init__.py\", line 49, in <module>\n",
            "    from pandas.core.api import (\n",
            "  File \"/home/manuel-bayona/Documents/crisil2/venv/lib/python3.12/site-packages/pandas/core/api.py\", line 9, in <module>\n",
            "    from pandas.core.dtypes.dtypes import (\n",
            "  File \"/home/manuel-bayona/Documents/crisil2/venv/lib/python3.12/site-packages/pandas/core/dtypes/dtypes.py\", line 24, in <module>\n",
            "    from pandas._libs import (\n",
            "  File \"/home/manuel-bayona/Documents/crisil2/venv/lib/python3.12/site-packages/pyarrow/__init__.py\", line 65, in <module>\n",
            "    import pyarrow.lib as _lib\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "_ARRAY_API not found",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
            "\u001b[31mAttributeError\u001b[39m: _ARRAY_API not found"
          ]
        }
      ],
      "source": [
        "# Standard libraries\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import logging\n",
        "from datetime import datetime, timedelta\n",
        "from typing import Dict, List, Optional, Tuple, Union, Any\n",
        "from pathlib import Path\n",
        "from dataclasses import dataclass, field\n",
        "\n",
        "# Data processing\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# PySpark\n",
        "from pyspark.sql import SparkSession, DataFrame as SparkDataFrame\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import StructType, StructField, StringType, FloatType, BooleanType, IntegerType, TimestampType\n",
        "\n",
        "# Great Expectations\n",
        "import great_expectations as ge\n",
        "from great_expectations.dataset import SparkDFDataset\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "logger = logging.getLogger('etl_pipeline')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/03/26 10:07:50 WARN Utils: Your hostname, manuel-bayona-System-Product-Name resolves to a loopback address: 127.0.1.1; using 192.168.0.3 instead (on interface enp5s0)\n",
            "25/03/26 10:07:50 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
            "25/03/26 10:08:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
          ]
        }
      ],
      "source": [
        "# Configuration Parameters\n",
        "INPUT_DIR = \"/home/manuel-bayona/Documents/crisil2/data\"\n",
        "OUTPUT_DIR = \"/home/manuel-bayona/Documents/crisil2/output\"\n",
        "EXPECTATIONS_PATH = \"/home/manuel-bayona/Documents/crisil2/critical_feature_expectations.json\"\n",
        "\n",
        "# Create Spark session\n",
        "def create_spark_session(app_name: str = \"RetailerETLPipeline\") -> SparkSession:\n",
        "    \"\"\"\n",
        "    Create and configure a Spark session.\n",
        "    \n",
        "    Args:\n",
        "        app_name: Name of the Spark application\n",
        "        \n",
        "    Returns:\n",
        "        Configured SparkSession\n",
        "    \"\"\"\n",
        "    return SparkSession.builder \\\n",
        "        .appName(app_name) \\\n",
        "        .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\") \\\n",
        "        .config(\"spark.sql.legacy.parquet.datetimeRebaseModeInWrite\", \"LEGACY\") \\\n",
        "        .config(\"spark.sql.legacy.parquet.datetimeRebaseModeInRead\", \"LEGACY\") \\\n",
        "        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
        "        .config(\"spark.driver.memory\", \"4g\") \\\n",
        "        .getOrCreate()\n",
        "\n",
        "# Initialize Spark\n",
        "spark = create_spark_session()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-03-26 10:08:06,803 - __main__.RetailerFileScanner - INFO - Found files for 3 retailers\n",
            "2025-03-26 10:08:06,803 - __main__.RetailerFileScanner - INFO -   - smartstopselfstorage: 3 files\n",
            "2025-03-26 10:08:06,803 - __main__.RetailerFileScanner - INFO -   - cubesmart: 3 files\n",
            "2025-03-26 10:08:06,804 - __main__.RetailerFileScanner - INFO -   - storagemart: 3 files\n",
            "2025-03-26 10:08:06,804 - __main__.RetailerFileScanner - INFO - Latest file for smartstopselfstorage: smartstopselfstorage_weekly_20250210_060127.parquet\n",
            "2025-03-26 10:08:06,804 - __main__.RetailerFileScanner - INFO - Latest file for cubesmart: cubesmart_weekly_20250210_061336.parquet\n",
            "2025-03-26 10:08:06,805 - __main__.RetailerFileScanner - INFO - Latest file for storagemart: storagemart_weekly_20250210_061303.parquet\n",
            "2025-03-26 10:08:06,805 - __main__.ETLProcessor - INFO - Processing file: smartstopselfstorage_weekly_20250210_060127.parquet\n",
            "25/03/26 10:08:06 WARN SQLConf: The SQL config 'spark.sql.legacy.parquet.datetimeRebaseModeInRead' has been deprecated in Spark v3.2 and may be removed in the future. Use 'spark.sql.parquet.datetimeRebaseModeInRead' instead.\n",
            "25/03/26 10:08:06 WARN SQLConf: The SQL config 'spark.sql.legacy.parquet.datetimeRebaseModeInWrite' has been deprecated in Spark v3.2 and may be removed in the future. Use 'spark.sql.parquet.datetimeRebaseModeInWrite' instead.\n",
            "25/03/26 10:08:06 WARN SQLConf: The SQL config 'spark.sql.legacy.parquet.datetimeRebaseModeInWrite' has been deprecated in Spark v3.2 and may be removed in the future. Use 'spark.sql.parquet.datetimeRebaseModeInWrite' instead.\n",
            "25/03/26 10:08:06 WARN SQLConf: The SQL config 'spark.sql.legacy.parquet.datetimeRebaseModeInRead' has been deprecated in Spark v3.2 and may be removed in the future. Use 'spark.sql.parquet.datetimeRebaseModeInRead' instead.\n",
            "25/03/26 10:08:06 WARN SQLConf: The SQL config 'spark.sql.legacy.parquet.datetimeRebaseModeInWrite' has been deprecated in Spark v3.2 and may be removed in the future. Use 'spark.sql.parquet.datetimeRebaseModeInWrite' instead.\n",
            "25/03/26 10:08:06 WARN SQLConf: The SQL config 'spark.sql.legacy.parquet.datetimeRebaseModeInRead' has been deprecated in Spark v3.2 and may be removed in the future. Use 'spark.sql.parquet.datetimeRebaseModeInRead' instead.\n",
            "2025-03-26 10:08:08,593 - __main__.SchemaManager - INFO - Schema validation passed\n",
            "2025-03-26 10:08:08,593 - __main__.MetadataTracker - INFO - No new columns detected in smartstopselfstorage_weekly_20250210_060127.parquet\n",
            "2025-03-26 10:08:08,594 - py.warnings - WARNING - /home/manuel-bayona/Documents/crisil2/venv/lib/python3.12/site-packages/pyspark/sql/pandas/utils.py:37: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "  if LooseVersion(pandas.__version__) < LooseVersion(minimum_pandas_version):\n",
            "\n",
            "2025-03-26 10:08:09,543 - py.warnings - WARNING - /home/manuel-bayona/Documents/crisil2/venv/lib/python3.12/site-packages/pyspark/sql/pandas/types.py:563: DeprecationWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
            "  if not is_datetime64tz_dtype(pser.dtype):\n",
            "\n",
            "2025-03-26 10:08:09,544 - py.warnings - WARNING - /home/manuel-bayona/Documents/crisil2/venv/lib/python3.12/site-packages/pyspark/sql/pandas/types.py:379: DeprecationWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
            "  if is_datetime64tz_dtype(s.dtype):\n",
            "\n",
            "2025-03-26 10:08:09,881 - __main__.ETLProcessor - INFO - Successfully processed smartstopselfstorage_weekly_20250210_060127.parquet\n",
            "2025-03-26 10:08:09,881 - __main__.ETLProcessor - INFO - Processing file: cubesmart_weekly_20250210_061336.parquet\n",
            "2025-03-26 10:08:09,955 - __main__.SchemaManager - INFO - Schema validation passed\n",
            "2025-03-26 10:08:09,955 - __main__.MetadataTracker - INFO - No new columns detected in cubesmart_weekly_20250210_061336.parquet\n",
            "2025-03-26 10:08:09,956 - py.warnings - WARNING - /home/manuel-bayona/Documents/crisil2/venv/lib/python3.12/site-packages/pyspark/sql/pandas/utils.py:37: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "  if LooseVersion(pandas.__version__) < LooseVersion(minimum_pandas_version):\n",
            "\n",
            "2025-03-26 10:08:10,690 - py.warnings - WARNING - /home/manuel-bayona/Documents/crisil2/venv/lib/python3.12/site-packages/pyspark/sql/pandas/types.py:563: DeprecationWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
            "  if not is_datetime64tz_dtype(pser.dtype):\n",
            "\n",
            "2025-03-26 10:08:10,690 - py.warnings - WARNING - /home/manuel-bayona/Documents/crisil2/venv/lib/python3.12/site-packages/pyspark/sql/pandas/types.py:379: DeprecationWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
            "  if is_datetime64tz_dtype(s.dtype):\n",
            "\n",
            "2025-03-26 10:08:11,122 - __main__.QualityChecker - WARNING - Failed expectation: ExpectColumnValuesToBeUnique for {'column': 'Product_ID'}\n",
            "2025-03-26 10:08:11,147 - __main__.ETLProcessor - WARNING - Filtering out 12 failing records from cubesmart_weekly_20250210_061336.parquet\n",
            "2025-03-26 10:08:11,148 - py.warnings - WARNING - /home/manuel-bayona/Documents/crisil2/venv/lib/python3.12/site-packages/pyspark/sql/pandas/utils.py:37: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "  if LooseVersion(pandas.__version__) < LooseVersion(minimum_pandas_version):\n",
            "\n",
            "2025-03-26 10:08:11,630 - py.warnings - WARNING - /home/manuel-bayona/Documents/crisil2/venv/lib/python3.12/site-packages/pyspark/sql/pandas/types.py:563: DeprecationWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
            "  if not is_datetime64tz_dtype(pser.dtype):\n",
            "\n",
            "2025-03-26 10:08:11,630 - py.warnings - WARNING - /home/manuel-bayona/Documents/crisil2/venv/lib/python3.12/site-packages/pyspark/sql/pandas/types.py:379: DeprecationWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
            "  if is_datetime64tz_dtype(s.dtype):\n",
            "\n",
            "2025-03-26 10:08:12,149 - py.warnings - WARNING - /home/manuel-bayona/Documents/crisil2/venv/lib/python3.12/site-packages/pyspark/sql/pandas/utils.py:37: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "  if LooseVersion(pandas.__version__) < LooseVersion(minimum_pandas_version):\n",
            "\n",
            "2025-03-26 10:08:12,150 - py.warnings - WARNING - /home/manuel-bayona/Documents/crisil2/venv/lib/python3.12/site-packages/pyspark/sql/pandas/types.py:379: DeprecationWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
            "  if is_datetime64tz_dtype(s.dtype):\n",
            "\n",
            "2025-03-26 10:08:13,276 - __main__.ETLProcessor - INFO - Successfully processed cubesmart_weekly_20250210_061336.parquet\n",
            "2025-03-26 10:08:13,292 - __main__.ETLProcessor - INFO - Processing file: storagemart_weekly_20250210_061303.parquet\n",
            "2025-03-26 10:08:13,353 - __main__.SchemaManager - INFO - Schema validation passed\n",
            "2025-03-26 10:08:13,354 - __main__.MetadataTracker - INFO - No new columns detected in storagemart_weekly_20250210_061303.parquet\n",
            "2025-03-26 10:08:13,354 - py.warnings - WARNING - /home/manuel-bayona/Documents/crisil2/venv/lib/python3.12/site-packages/pyspark/sql/pandas/utils.py:37: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "  if LooseVersion(pandas.__version__) < LooseVersion(minimum_pandas_version):\n",
            "\n",
            "2025-03-26 10:08:13,547 - py.warnings - WARNING - /home/manuel-bayona/Documents/crisil2/venv/lib/python3.12/site-packages/pyspark/sql/pandas/types.py:563: DeprecationWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
            "  if not is_datetime64tz_dtype(pser.dtype):\n",
            "\n",
            "2025-03-26 10:08:13,547 - py.warnings - WARNING - /home/manuel-bayona/Documents/crisil2/venv/lib/python3.12/site-packages/pyspark/sql/pandas/types.py:379: DeprecationWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
            "  if is_datetime64tz_dtype(s.dtype):\n",
            "\n",
            "2025-03-26 10:08:13,804 - __main__.ETLProcessor - INFO - Successfully processed storagemart_weekly_20250210_061303.parquet\n",
            "25/03/26 10:08:14 WARN TaskSetManager: Stage 7 contains a task of very large size (1852 KiB). The maximum recommended task size is 1000 KiB.\n",
            "2025-03-26 10:08:15,349 - __main__.ETLProcessor - INFO - Appended data to existing output table: /home/manuel-bayona/Documents/crisil2/output/unified_retailer_data.parquet\n",
            "2025-03-26 10:08:15,349 - __main__.ETLProcessor - INFO - ETL pipeline completed successfully\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ETL pipeline completed successfully\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/03/26 10:08:16 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
          ]
        }
      ],
      "source": [
        "@dataclass\n",
        "class FileMetadata:\n",
        "    \"\"\"Class to store metadata about a retailer data file.\"\"\"\n",
        "    file_path: str\n",
        "    retailer: str\n",
        "    date: datetime\n",
        "    timestamp: str\n",
        "    \n",
        "    @property\n",
        "    def filename(self) -> str:\n",
        "        \"\"\"Return just the filename without the path.\"\"\"\n",
        "        return os.path.basename(self.file_path)\n",
        "\n",
        "\n",
        "class RetailerFileScanner:\n",
        "    \"\"\"\n",
        "    Scans directories for retailer data files matching specific patterns.\n",
        "    \n",
        "    This class handles the identification of retailer-specific files based on\n",
        "    naming conventions and extracts metadata from filenames.\n",
        "    \"\"\"\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    def __init__(self, input_dir: str):\n",
        "        \"\"\"\n",
        "        Initialize the file scanner.\n",
        "        \n",
        "        Args:\n",
        "            input_dir: Directory containing retailer data files\n",
        "        \"\"\"\n",
        "        # Regex pattern for retailer files\n",
        "        self.file_pattern = r\"^(cubesmart|smartstopselfstorage|storagemart)_weekly_(\\d{8})_(\\d{6})\\.parquet$\"\n",
        "        self.input_dir = input_dir\n",
        "        self.logger = logging.getLogger(f\"{__name__}.RetailerFileScanner\")\n",
        "    \n",
        "    def scan_files(self) -> Dict[str, List[FileMetadata]]:\n",
        "        \"\"\"\n",
        "        Scan the input directory for retailer files.\n",
        "        \n",
        "        Returns:\n",
        "            Dictionary mapping retailer names to lists of file metadata\n",
        "        \"\"\"\n",
        "        retailer_files: Dict[str, List[FileMetadata]] = {}\n",
        "        \n",
        "        try:\n",
        "            for filename in os.listdir(self.input_dir):\n",
        "                match = re.match(self.file_pattern, filename)\n",
        "                if match:\n",
        "                    retailer, date_str, time_str = match.groups()\n",
        "                    \n",
        "                    # Parse date and time\n",
        "                    file_date = datetime.strptime(date_str, \"%Y%m%d\")\n",
        "                    \n",
        "                    # Create file metadata\n",
        "                    file_path = os.path.join(self.input_dir, filename)\n",
        "                    metadata = FileMetadata(\n",
        "                        file_path=file_path,\n",
        "                        retailer=retailer,\n",
        "                        date=file_date,\n",
        "                        timestamp=time_str\n",
        "                    )\n",
        "                    \n",
        "                    # Add to retailer files dictionary\n",
        "                    if retailer not in retailer_files:\n",
        "                        retailer_files[retailer] = []\n",
        "                    retailer_files[retailer].append(metadata)\n",
        "            \n",
        "            # Sort files by date (newest first) for each retailer\n",
        "            for retailer in retailer_files:\n",
        "                retailer_files[retailer].sort(key=lambda x: x.date, reverse=True)\n",
        "            \n",
        "            self.logger.info(f\"Found files for {len(retailer_files)} retailers\")\n",
        "            for retailer, files in retailer_files.items():\n",
        "                self.logger.info(f\"  - {retailer}: {len(files)} files\")\n",
        "            \n",
        "            return retailer_files\n",
        "        \n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error scanning files: {str(e)}\")\n",
        "            raise\n",
        "    \n",
        "    def get_latest_files(self, reference_date: Optional[datetime] = None) -> Dict[str, FileMetadata]:\n",
        "        \"\"\"\n",
        "        Get the latest file for each retailer.\n",
        "        \n",
        "        Args:\n",
        "            reference_date: Reference date to find the latest file before this date.\n",
        "                           If None, uses current date minus one day.\n",
        "        \n",
        "        Returns:\n",
        "            Dictionary mapping retailer names to their latest file metadata\n",
        "        \"\"\"\n",
        "        if reference_date is None:\n",
        "            reference_date = datetime.now() - timedelta(days=1)\n",
        "        \n",
        "        retailer_files = self.scan_files()\n",
        "        latest_files = {}\n",
        "        \n",
        "        for retailer, files in retailer_files.items():\n",
        "            # Filter files before reference date\n",
        "            valid_files = [f for f in files if f.date <= reference_date]\n",
        "            \n",
        "            if valid_files:\n",
        "                # Get the most recent file\n",
        "                latest_files[retailer] = valid_files[0]\n",
        "                self.logger.info(f\"Latest file for {retailer}: {latest_files[retailer].filename}\")\n",
        "            else:\n",
        "                self.logger.warning(f\"No valid files found for {retailer} before {reference_date}\")\n",
        "        \n",
        "        return latest_files\n",
        "\n",
        "\n",
        "class SchemaManager:\n",
        "    \"\"\"\n",
        "    Manages schema definitions and validation for retailer data.\n",
        "    \n",
        "    This class handles schema consistency checks and provides the canonical\n",
        "    schema definition for the unified output.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize the schema manager with canonical schema definitions.\"\"\"\n",
        "        self.logger = logging.getLogger(f\"{__name__}.SchemaManager\")\n",
        "        \n",
        "        # Define canonical schema for PySpark\n",
        "        self.canonical_schema = StructType([\n",
        "            StructField(\"Run_Date\", TimestampType(), False),\n",
        "            StructField(\"Retailer\", StringType(), False),\n",
        "            StructField(\"Retailer_Website\", StringType(), True),\n",
        "            StructField(\"Store_Info\", StringType(), False),\n",
        "            StructField(\"Category_Traversal\", StringType(), False),\n",
        "            StructField(\"Brand\", StringType(), False),\n",
        "            StructField(\"Product_Name\", StringType(), False),\n",
        "            StructField(\"Product_ID\", StringType(), False),\n",
        "            StructField(\"Universal_Product_ID\", StringType(), True),\n",
        "            StructField(\"Variant_Info\", StringType(), False),\n",
        "            StructField(\"Size_Or_Quantity\", StringType(), False),\n",
        "            StructField(\"Number_Of_Reviews\", FloatType(), True),\n",
        "            StructField(\"Average_Rating\", FloatType(), True),\n",
        "            StructField(\"Original_Price\", FloatType(), True),\n",
        "            StructField(\"Sale_Price\", FloatType(), True),\n",
        "            StructField(\"Currency\", StringType(), True),\n",
        "            StructField(\"Url\", StringType(), True),\n",
        "            StructField(\"In_Stock\", BooleanType(), False)\n",
        "        ])\n",
        "        \n",
        "        # List of required columns that must be present\n",
        "        self.required_columns = [\n",
        "            \"Run_Date\", \"Retailer\", \"Store_Info\", \"Category_Traversal\",\n",
        "            \"Brand\", \"Product_Name\", \"Product_ID\", \"Variant_Info\",\n",
        "            \"Size_Or_Quantity\", \"In_Stock\"\n",
        "        ]\n",
        "    \n",
        "    def validate_schema(self, df: SparkDataFrame) -> Tuple[bool, List[str]]:\n",
        "        \"\"\"\n",
        "        Validate that a dataframe has all required columns.\n",
        "        \n",
        "        Args:\n",
        "            df: Spark DataFrame to validate\n",
        "            \n",
        "        Returns:\n",
        "            Tuple of (is_valid, missing_columns)\n",
        "        \"\"\"\n",
        "        df_columns = set(df.columns)\n",
        "        required_columns = set(self.required_columns)\n",
        "        \n",
        "        missing_columns = required_columns - df_columns\n",
        "        \n",
        "        is_valid = len(missing_columns) == 0\n",
        "        \n",
        "        if not is_valid:\n",
        "            self.logger.warning(f\"Schema validation failed. Missing columns: {missing_columns}\")\n",
        "        else:\n",
        "            self.logger.info(\"Schema validation passed\")\n",
        "        \n",
        "        return is_valid, list(missing_columns)\n",
        "    \n",
        "    def detect_new_columns(self, df: SparkDataFrame) -> List[str]:\n",
        "        \"\"\"\n",
        "        Detect columns in the dataframe that are not in the canonical schema.\n",
        "        \n",
        "        Args:\n",
        "            df: Spark DataFrame to check\n",
        "            \n",
        "        Returns:\n",
        "            List of new column names\n",
        "        \"\"\"\n",
        "        canonical_columns = {field.name for field in self.canonical_schema.fields}\n",
        "        df_columns = set(df.columns)\n",
        "        \n",
        "        new_columns = df_columns - canonical_columns\n",
        "        \n",
        "        if new_columns:\n",
        "            self.logger.info(f\"Detected {len(new_columns)} new columns: {new_columns}\")\n",
        "        \n",
        "        return list(new_columns)\n",
        "    \n",
        "    def get_canonical_columns(self) -> List[str]:\n",
        "        \"\"\"\n",
        "        Get the list of canonical column names.\n",
        "        \n",
        "        Returns:\n",
        "            List of column names in the canonical schema\n",
        "        \"\"\"\n",
        "        return [field.name for field in self.canonical_schema.fields]\n",
        "\n",
        "\n",
        "class QualityChecker:\n",
        "    \"\"\"\n",
        "    Performs quality checks on retailer data using Great Expectations.\n",
        "    \n",
        "    This class applies data quality rules defined in a JSON configuration file\n",
        "    and reports on validation results.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, expectations_path: str):\n",
        "        \"\"\"\n",
        "        Initialize the quality checker.\n",
        "        \n",
        "        Args:\n",
        "            expectations_path: Path to the JSON file containing expectations\n",
        "        \"\"\"\n",
        "        self.logger = logging.getLogger(f\"{__name__}.QualityChecker\")\n",
        "        self.expectations_path = expectations_path\n",
        "        self.expectations = self._load_expectations()\n",
        "    \n",
        "    def _load_expectations(self) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Load expectations from the JSON configuration file.\n",
        "        \n",
        "        Returns:\n",
        "            List of expectation configurations\n",
        "        \"\"\"\n",
        "        try:\n",
        "            with open(self.expectations_path, 'r') as f:\n",
        "                config = json.load(f)\n",
        "                return config.get('expectations', [])\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Failed to load expectations: {str(e)}\")\n",
        "            raise\n",
        "    \n",
        "    def validate_dataframe(self, df: SparkDataFrame) -> Tuple[bool, Dict[str, Any], List[int]]:\n",
        "        try:\n",
        "            pandas_df = df.toPandas()\n",
        "            ge_df = ge.from_pandas(pandas_df)\n",
        "            \n",
        "            validation_results = {\"success\": True, \"results\": []}\n",
        "            failing_indices = set()\n",
        "\n",
        "            for expectation in self.expectations:\n",
        "                expectation_type = expectation.get('expectation_type')\n",
        "                kwargs = expectation.get('kwargs', {})\n",
        "                column = kwargs.get('column')\n",
        "\n",
        "                if column and column not in df.columns:\n",
        "                    self.logger.warning(f\"Column '{column}' not found, skipping expectation: {expectation_type}\")\n",
        "                    continue\n",
        "\n",
        "                try:\n",
        "                    if expectation_type == \"ExpectColumnValuesToNotBeNull\":\n",
        "                        result = ge_df.expect_column_values_to_not_be_null(**kwargs)\n",
        "                        if not result[\"success\"]:\n",
        "                            failed_rows = pandas_df[pandas_df[column].isnull()]\n",
        "                            failing_indices.update(failed_rows.index.tolist())\n",
        "\n",
        "                    elif expectation_type == \"ExpectColumnValuesToBeUnique\":\n",
        "                        duplicates = pandas_df[pandas_df.duplicated(column, keep=False)]\n",
        "                        result = ge_df.expect_column_values_to_be_unique(**kwargs)\n",
        "                        if not result[\"success\"]:\n",
        "                            failing_indices.update(duplicates.index.tolist())\n",
        "\n",
        "                    elif expectation_type == \"ExpectColumnValuesToBeBetween\":\n",
        "                        min_val = kwargs.get(\"min_value\", float('-inf'))\n",
        "                        max_val = kwargs.get(\"max_value\", float('inf'))\n",
        "                        result = ge_df.expect_column_values_to_be_between(**kwargs)\n",
        "                        if not result[\"success\"]:\n",
        "                            failed_rows = pandas_df[(pandas_df[column] < min_val) | (pandas_df[column] > max_val)]\n",
        "                            failing_indices.update(failed_rows.index.tolist())\n",
        "\n",
        "                    else:\n",
        "                        self.logger.warning(f\"Unsupported expectation type: {expectation_type}\")\n",
        "                        continue\n",
        "\n",
        "                    validation_results[\"results\"].append(result)\n",
        "\n",
        "                    if not result[\"success\"]:\n",
        "                        validation_results[\"success\"] = False\n",
        "                        self.logger.warning(f\"Failed expectation: {expectation_type} for {kwargs}\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    self.logger.error(f\"Error applying expectation {expectation_type}: {str(e)}\")\n",
        "                    validation_results[\"success\"] = False\n",
        "                    validation_results[\"results\"].append({\n",
        "                        \"expectation_type\": expectation_type,\n",
        "                        \"kwargs\": kwargs,\n",
        "                        \"success\": False,\n",
        "                        \"error\": str(e)\n",
        "                    })\n",
        "\n",
        "            return validation_results[\"success\"], validation_results, list(failing_indices)\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error during validation: {str(e)}\")\n",
        "            return False, {\"success\": False, \"error\": str(e)}, []\n",
        "\n",
        "\n",
        "\n",
        "class MetadataTracker:\n",
        "    \"\"\"\n",
        "    Tracks metadata about processed files and schema changes.\n",
        "    \n",
        "    This class logs information about newly detected columns and processing\n",
        "    statistics to enable monitoring and analysis.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, output_dir: str):\n",
        "        \"\"\"\n",
        "        Initialize the metadata tracker.\n",
        "        \n",
        "        Args:\n",
        "            output_dir: Directory to store metadata logs\n",
        "        \"\"\"\n",
        "        self.logger = logging.getLogger(f\"{__name__}.MetadataTracker\")\n",
        "        self.output_dir = output_dir\n",
        "        self.metadata_file = os.path.join(output_dir, \"column_metadata.parquet\")\n",
        "        \n",
        "        # Create output directory if it doesn't exist\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "    \n",
        "    def log_new_columns(self, file_name: str, new_columns: List[str], spark: SparkSession) -> None:\n",
        "        \"\"\"\n",
        "        Log newly detected columns to the metadata table.\n",
        "        \n",
        "        Args:\n",
        "            file_name: Name of the source file\n",
        "            new_columns: List of new column names\n",
        "            spark: SparkSession to use for writing\n",
        "        \"\"\"\n",
        "        if not new_columns:\n",
        "            self.logger.info(f\"No new columns detected in {file_name}\")\n",
        "            return\n",
        "        \n",
        "        # Create a dataframe with the new column information\n",
        "        data = [(file_name, col, datetime.now()) for col in new_columns]\n",
        "        schema = StructType([\n",
        "            StructField(\"file_name\", StringType(), False),\n",
        "            StructField(\"new_column\", StringType(), False),\n",
        "            StructField(\"detection_time\", TimestampType(), False)\n",
        "        ])\n",
        "        \n",
        "        new_columns_df = spark.createDataFrame(data, schema)\n",
        "        \n",
        "        # Append to existing metadata file if it exists, otherwise create it\n",
        "        if os.path.exists(self.metadata_file):\n",
        "            existing_df = spark.read.parquet(self.metadata_file)\n",
        "            combined_df = existing_df.union(new_columns_df)\n",
        "            combined_df.write.mode(\"overwrite\").parquet(self.metadata_file)\n",
        "        else:\n",
        "            new_columns_df.write.mode(\"overwrite\").parquet(self.metadata_file)\n",
        "        \n",
        "        self.logger.info(f\"Logged {len(new_columns)} new columns for {file_name}\")\n",
        "    \n",
        "    def get_column_metadata(self, spark: SparkSession) -> Optional[SparkDataFrame]:\n",
        "        \"\"\"\n",
        "        Retrieve the column metadata table.\n",
        "        \n",
        "        Args:\n",
        "            spark: SparkSession to use for reading\n",
        "            \n",
        "        Returns:\n",
        "            Spark DataFrame containing column metadata, or None if not found\n",
        "        \"\"\"\n",
        "        if os.path.exists(self.metadata_file):\n",
        "            return spark.read.parquet(self.metadata_file)\n",
        "        return None\n",
        "\n",
        "\n",
        "class ETLProcessor:\n",
        "    \"\"\"\n",
        "    Main ETL processor that orchestrates the entire pipeline.\n",
        "    \n",
        "    This class coordinates file scanning, validation, quality checks,\n",
        "    transformation, and output writing.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        input_dir: str,\n",
        "        output_dir: str,\n",
        "        expectations_path: str,\n",
        "        spark: Optional[SparkSession] = None\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialize the ETL processor.\n",
        "        \n",
        "        Args:\n",
        "            input_dir: Directory containing input files\n",
        "            output_dir: Directory for output files and metadata\n",
        "            expectations_path: Path to expectations JSON file\n",
        "            spark: Optional SparkSession (will create one if not provided)\n",
        "        \"\"\"\n",
        "        self.logger = logging.getLogger(f\"{__name__}.ETLProcessor\")\n",
        "        self.input_dir = input_dir\n",
        "        self.output_dir = output_dir\n",
        "        self.expectations_path = expectations_path\n",
        "        \n",
        "        # Create output directory if it doesn't exist\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        \n",
        "        # Initialize Spark session if not provided\n",
        "        self.spark = spark if spark else create_spark_session()\n",
        "        \n",
        "        # Initialize components\n",
        "        self.file_scanner = RetailerFileScanner(input_dir)\n",
        "        self.schema_manager = SchemaManager()\n",
        "        self.quality_checker = QualityChecker(expectations_path)\n",
        "        self.metadata_tracker = MetadataTracker(output_dir)\n",
        "        \n",
        "        # Output table path\n",
        "        self.output_table_path = os.path.join(output_dir, \"unified_retailer_data.parquet\")\n",
        "\n",
        "    def _cast_pandas_to_canonical_types(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Cast pandas DataFrame columns to match the canonical Spark schema.\n",
        "        Handles Spark-to-Pandas type translation for safe conversion.\n",
        "        \"\"\"\n",
        "        for field in self.schema_manager.canonical_schema.fields:\n",
        "            col_name = field.name\n",
        "            if col_name not in df.columns:\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                spark_type = field.dataType\n",
        "\n",
        "                if isinstance(spark_type, FloatType):\n",
        "                    df[col_name] = pd.to_numeric(df[col_name], errors='coerce').astype(float)\n",
        "\n",
        "                elif isinstance(spark_type, IntegerType):\n",
        "                    # Use nullable pandas Int64 to support missing values\n",
        "                    df[col_name] = pd.to_numeric(df[col_name], errors='coerce').astype('Int64')\n",
        "\n",
        "                elif isinstance(spark_type, BooleanType):\n",
        "                    df[col_name] = df[col_name].astype(bool)\n",
        "\n",
        "                elif isinstance(spark_type, TimestampType):\n",
        "                    df[col_name] = pd.to_datetime(df[col_name], errors='coerce')\n",
        "\n",
        "                elif isinstance(spark_type, StringType):\n",
        "                    # Explicitly cast to string to handle mixed object types\n",
        "                    df[col_name] = df[col_name].astype(str)\n",
        "\n",
        "                else:\n",
        "                    self.logger.warning(f\"Unhandled Spark type for column {col_name}: {type(spark_type)}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                self.logger.warning(f\"Failed to cast column {col_name}: {str(e)}\")\n",
        "\n",
        "        return df\n",
        "  \n",
        "    def process_file(self, file_metadata: FileMetadata) -> Optional[SparkDataFrame]:\n",
        "        \"\"\"\n",
        "        Process a single retailer file through the ETL pipeline.\n",
        "        \n",
        "        Args:\n",
        "            file_metadata: Metadata for the file to process\n",
        "            \n",
        "        Returns:\n",
        "            Processed Spark DataFrame, or None if processing failed\n",
        "        \"\"\"\n",
        "        file_path = file_metadata.file_path\n",
        "        retailer = file_metadata.retailer\n",
        "        \n",
        "        try:\n",
        "            # Read the file\n",
        "            self.logger.info(f\"Processing file: {file_metadata.filename}\")\n",
        "            df = self.spark.read.parquet(file_path)\n",
        "            \n",
        "            # Validate schema\n",
        "            is_valid, missing_columns = self.schema_manager.validate_schema(df)\n",
        "            if not is_valid:\n",
        "                self.logger.error(f\"Schema validation failed for {file_metadata.filename}. Missing columns: {missing_columns}\")\n",
        "                return None\n",
        "            \n",
        "            # Detect new columns for metadata tracking\n",
        "            new_columns = self.schema_manager.detect_new_columns(df)\n",
        "            self.metadata_tracker.log_new_columns(file_metadata.filename, new_columns, self.spark)\n",
        "            \n",
        "            # Perform quality checks\n",
        "            passed_qa, qa_results, failing_indices = self.quality_checker.validate_dataframe(df)\n",
        "\n",
        "            if failing_indices:\n",
        "                self.logger.warning(f\"Filtering out {len(failing_indices)} failing records from {file_metadata.filename}\")\n",
        "                pandas_df = df.toPandas()\n",
        "                valid_df = pandas_df.drop(index=failing_indices)\n",
        "\n",
        "                if valid_df.empty:\n",
        "                    self.logger.warning(f\"All records failed quality checks in {file_metadata.filename}\")\n",
        "                    return None\n",
        "\n",
        "                # NEW: cast pandas columns to canonical types\n",
        "                valid_df = self._cast_pandas_to_canonical_types(valid_df)\n",
        "\n",
        "                df = self.spark.createDataFrame(valid_df, schema=self.schema_manager.canonical_schema)\n",
        "\n",
        "            elif not passed_qa:\n",
        "                self.logger.error(f\"Quality checks failed entirely for {file_metadata.filename}\")\n",
        "                return None\n",
        "            \n",
        "            # Normalize data types\n",
        "            normalized_df = self._normalize_dataframe(df)\n",
        "            \n",
        "            # Select only canonical columns\n",
        "            canonical_columns = self.schema_manager.get_canonical_columns()\n",
        "            output_df = normalized_df.select(*[col for col in canonical_columns if col in normalized_df.columns])\n",
        "            \n",
        "            self.logger.info(f\"Successfully processed {file_metadata.filename}\")\n",
        "            return output_df\n",
        "        \n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error processing file {file_metadata.filename}: {str(e)}\")\n",
        "            return None\n",
        "    \n",
        "    def _normalize_dataframe(self, df: SparkDataFrame) -> SparkDataFrame:\n",
        "        \"\"\"\n",
        "        Normalize data types in the dataframe to match canonical schema.\n",
        "        \n",
        "        Args:\n",
        "            df: Spark DataFrame to normalize\n",
        "            \n",
        "        Returns:\n",
        "            Normalized Spark DataFrame\n",
        "        \"\"\"\n",
        "        normalized_df = df\n",
        "        \n",
        "        # Apply type conversions\n",
        "        for field in self.schema_manager.canonical_schema.fields:\n",
        "            if field.name in df.columns:\n",
        "                col_name = field.name\n",
        "                \n",
        "                if isinstance(field.dataType, TimestampType):\n",
        "                    normalized_df = normalized_df.withColumn(\n",
        "                        col_name, \n",
        "                        F.to_timestamp(F.col(col_name))\n",
        "                    )\n",
        "                elif isinstance(field.dataType, FloatType):\n",
        "                    normalized_df = normalized_df.withColumn(\n",
        "                        col_name,\n",
        "                        F.col(col_name).cast(\"float\")\n",
        "                    )\n",
        "                elif isinstance(field.dataType, BooleanType):\n",
        "                    normalized_df = normalized_df.withColumn(\n",
        "                        col_name,\n",
        "                        F.col(col_name).cast(\"boolean\")\n",
        "                    )\n",
        "                elif isinstance(field.dataType, StringType):\n",
        "                    normalized_df = normalized_df.withColumn(\n",
        "                        col_name,\n",
        "                        F.col(col_name).cast(\"string\")\n",
        "                    )\n",
        "        \n",
        "        return normalized_df\n",
        "    \n",
        "    def run_pipeline(self, reference_date: Optional[datetime] = None) -> bool:\n",
        "        \"\"\"\n",
        "        Run the complete ETL pipeline.\n",
        "        \n",
        "        Args:\n",
        "            reference_date: Reference date for finding latest files\n",
        "            \n",
        "        Returns:\n",
        "            True if pipeline completed successfully, False otherwise\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Get latest files for each retailer\n",
        "            latest_files = self.file_scanner.get_latest_files(reference_date)\n",
        "            \n",
        "            if not latest_files:\n",
        "                self.logger.warning(\"No files found to process\")\n",
        "                return False\n",
        "            \n",
        "            # Process each file\n",
        "            processed_dfs = []\n",
        "            for retailer, file_metadata in latest_files.items():\n",
        "                processed_df = self.process_file(file_metadata)\n",
        "                if processed_df:\n",
        "                    processed_dfs.append(processed_df)\n",
        "            \n",
        "            if not processed_dfs:\n",
        "                self.logger.error(\"No files were successfully processed\")\n",
        "                return False\n",
        "            \n",
        "            # Combine all processed dataframes\n",
        "            if len(processed_dfs) > 1:\n",
        "                combined_df = processed_dfs[0]\n",
        "                for df in processed_dfs[1:]:\n",
        "                    combined_df = combined_df.union(df)\n",
        "            else:\n",
        "                combined_df = processed_dfs[0]\n",
        "            \n",
        "            # Write to output table\n",
        "            self._write_to_output_table(combined_df)\n",
        "            \n",
        "            self.logger.info(\"ETL pipeline completed successfully\")\n",
        "            return True\n",
        "        \n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error running ETL pipeline: {str(e)}\")\n",
        "            return False\n",
        "    \n",
        "    def _write_to_output_table(self, df: SparkDataFrame) -> None:\n",
        "        \"\"\"\n",
        "        Write processed data to the unified output table.\n",
        "        \n",
        "        Args:\n",
        "            df: Spark DataFrame to write\n",
        "        \"\"\"\n",
        "        # Check if output table exists\n",
        "        output_exists = os.path.exists(self.output_table_path)\n",
        "        \n",
        "        if output_exists:\n",
        "            # Append to existing table\n",
        "            df.write.mode(\"append\").parquet(self.output_table_path)\n",
        "            self.logger.info(f\"Appended data to existing output table: {self.output_table_path}\")\n",
        "        else:\n",
        "            # Create new table\n",
        "            df.write.mode(\"overwrite\").parquet(self.output_table_path)\n",
        "            self.logger.info(f\"Created new output table: {self.output_table_path}\")\n",
        "\n",
        "\n",
        "# Example usage\n",
        "\n",
        "\n",
        "# Initialize and run ETL pipeline\n",
        "etl = ETLProcessor(\n",
        "    input_dir=INPUT_DIR,\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    expectations_path=EXPECTATIONS_PATH,\n",
        "    spark=spark\n",
        ")\n",
        "\n",
        "success = etl.run_pipeline()\n",
        "\n",
        "if success:\n",
        "    print(\"ETL pipeline completed successfully\")\n",
        "else:\n",
        "    print(\"ETL pipeline failed\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
